# ğŸ”¥ Classification Algorithms From Scratch â€” Perceptron, LDA & Gaussian Naive Bayes

This repository implements **classic machine learning classifiers from scratch**, without using scikit-learn models.  
The project includes:

- Perceptron classifier  
- Linear Discriminant Analysis (LDA) classifier  
- Gaussian Naive Bayes classifier  
- Custom loss functions  
- Evaluation & visualization scripts  
- A theoretical handwritten PDF with derivations

The classifiers are tested on:

- Linearly separable vs. linearly inseparable datasets  
- Gaussian mixture datasets  
- Decision boundaries, covariances, and accuracy comparisons  
- Perceptron training dynamics

---

## ğŸ“ Project Structure

```
ml-classifiers-from-scratch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ base_estimator.py          # Core estimator interface
â”‚   â”œâ”€â”€ classifiers.py             # Perceptron, LDA, GaussianNB implementations
â”‚   â”œâ”€â”€ classifiers_evaluation.py  # Experiment runner + plotting
â”‚   â”œâ”€â”€ loss_functions.py          # misclassification error + accuracy
â”‚   â”œâ”€â”€ utils.py                   # Plotly helpers, decision surfaces, ellipses
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ gaussian1.npy
â”‚   â”œâ”€â”€ gaussian2.npy
â”‚   â”œâ”€â”€ linearly_separable.npy
â”‚   â”œâ”€â”€ linearly_inseparable.npy
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Answers.pdf                # Mathematical derivations & theory
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Implemented Models

### ğŸ”¹ **BaseEstimator**  
`src/base_estimator.py`

Imitates scikit-learn's API and defines:

- `fit(X, y)`
- `predict(X)`
- `loss(X, y)`
- `fit_predict(X, y)`

It forces every classifier to implement `_fit`, `_predict`, `_loss`.

---

### ğŸ”¹ **Perceptron Classifier**  
`src/classifiers.py`

Features:

- Online learning  
- Supports intercept  
- Iterates until convergence or max iterations  
- Callback after every update for tracking loss  
- Works on both separable and inseparable datasets

---

### ğŸ”¹ **LDA (Linear Discriminant Analysis)**  
`src/classifiers.py`

Implements:

- Class means  
- Shared covariance matrix  
- Class priors  
- Gaussian likelihood  
- Discriminant functions  
- Full covariance ellipse visualization  

---

### ğŸ”¹ **Gaussian Naive Bayes**  
`src/classifiers.py`

Implements:

- Per-class feature means  
- Per-class variances  
- Independence assumption  
- Closedâ€‘form likelihood  
- Diagonal covariance ellipses  

---

## ğŸ“Š Evaluation & Visualization

### **1. Perceptron Training Loss**
```bash
python src/classifiers_evaluation.py
```

Generates training loss curves for:

- `linearly_separable.npy`  
- `linearly_inseparable.npy`

---

### **2. LDA vs Gaussian NB Comparison**

Also via:
```bash
python src/classifiers_evaluation.py
```

For:

- `gaussian1.npy`
- `gaussian2.npy`

Includes:

- Two subplots (LDA vs GNB)  
- Predicted class coloring  
- True label marker shapes  
- Class mean markers  
- Covariance ellipses  
- Printed accuracies  

---

## ğŸ§  Theoretical PDF

`docs/Answers.pdf` includes:

- Likelihood derivations  
- Gaussian / Poisson / Multinomial models  
- MLE estimators  
- LDA discriminant rule  
- Naive Bayes assumptions  
- Perceptron separability theory  

---

## ğŸ“¦ Installation

```
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Perceptron experiment:
```
python src/classifiers_evaluation.py
```

### Gaussian classifier comparison:
```
python src/classifiers_evaluation.py
```

### Import models manually:
```python
from src.classifiers import Perceptron, LDA, GaussianNaiveBayes
```

---

## ğŸ›  Technologies

- Python  
- NumPy  
- Plotly  
- Matplotlib  

---

## ğŸ¯ Learning Outcomes

- Implement ML models manually  
- Understand generative & discriminative models  
- Visualize classifier behavior  
- Build scikitâ€‘learnâ€“style class architecture  
- Work with Gaussian likelihoods  

---

## ğŸ“˜ License
MIT License.

---

## ğŸ™Œ Author  
**Yair Mahfud**
